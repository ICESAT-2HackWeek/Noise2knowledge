{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate the Ice-Ocean Transition Across ICESat-2 Datasets\n",
    "\n",
    "## Motivating Questions\n",
    "- What does the ice-ocean transition zone look like across data products?\n",
    "- How does the presence of sea ice versus open water impact the profile?\n",
    "\n",
    "## Region of Interest\n",
    "- Greenland Marine Terminating Glacier\n",
    "\n",
    "## Code Resources\n",
    "- NSIDC DAAC ICESat-2 Customize and Access Jupyter Notebook by Amy Steiker from ICESat-2 Hackweek\n",
    "\n",
    "## Data Resources\n",
    "- ICESat-2 ATL06 and ATL07/ATL10 data from the NSIDC DAAC (https://doi.org/10.5067/ATLAS/ATL06.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import getpass\n",
    "import socket\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import pprint\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import fiona\n",
    "import h5py\n",
    "import re\n",
    "# To read KML files with geopandas, we will need to enable KML support in fiona (disabled by default)\n",
    "fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from shapely.geometry.polygon import orient\n",
    "from statistics import mean\n",
    "from requests.auth import HTTPBasicAuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earthdata Login password: ········\n"
     ]
    }
   ],
   "source": [
    "# # Earthdata Login credentials\n",
    "\n",
    "# # Enter your Earthdata Login user name\n",
    "# uid = 'jessica.scheick'\n",
    "# # Enter your email address associated with your Earthdata Login account\n",
    "# email = 'jessica.scheick@maine.edu'\n",
    "# pswd = getpass.getpass('Earthdata Login password: ')\n",
    "# Earthdata Login credentials\n",
    "\n",
    "# Enter your Earthdata Login user name\n",
    "uid = 'norlandrhagen'\n",
    "# Enter your email address associated with your Earthdata Login account\n",
    "email = 'norlandrhagen@gmail.com'\n",
    "pswd = getpass.getpass('Earthdata Login password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4F357936-121C-5B23-46DE-94786799A20B\n"
     ]
    }
   ],
   "source": [
    "# Request token from Common Metadata Repository using Earthdata credentials\n",
    "token_api_url = 'https://cmr.earthdata.nasa.gov/legacy-services/rest/tokens'\n",
    "hostname = socket.gethostname()\n",
    "ip = socket.gethostbyname(hostname)\n",
    "\n",
    "data = {\n",
    "    'token': {\n",
    "        'username': uid,\n",
    "        'password': pswd,\n",
    "        'client_id': 'NSIDC_client_id',\n",
    "        'user_ip_address': ip\n",
    "    }\n",
    "}\n",
    "headers={'Accept': 'application/json'}\n",
    "response = requests.post(token_api_url, json=data, headers=headers)\n",
    "token = json.loads(response.content)['token']['id']\n",
    "print(token)\n",
    "#4351E4A7-9907-AEA1-82B3-FE2A36C88659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data set ID (e.g. ATL06) of interest here, also known as \"short name\".\n",
    "\n",
    "short_name = 'ATL06'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the number and size of granules available within a time range and location.\n",
    "\n",
    "#### Let's explore information about our data set. We'll start by determining the most recent version number of our data set. We will also find out how many data granules (files) exist over an area and time of interest. [The Common Metadata Repository](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html \"CMR API documentation\") is queried to explore this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feed': {'entry': [{'archive_center': 'NASA NSIDC DAAC',\n",
      "                     'associations': {'services': ['S1568899363-NSIDC_ECS',\n",
      "                                                   'S1613689509-NSIDC_ECS',\n",
      "                                                   'S1613669681-NSIDC_ECS']},\n",
      "                     'boxes': ['-90 -180 90 180'],\n",
      "                     'browse_flag': False,\n",
      "                     'coordinate_system': 'CARTESIAN',\n",
      "                     'data_center': 'NSIDC_ECS',\n",
      "                     'dataset_id': 'ATLAS/ICESat-2 L3A Land Ice Height V001',\n",
      "                     'has_formats': True,\n",
      "                     'has_spatial_subsetting': True,\n",
      "                     'has_temporal_subsetting': True,\n",
      "                     'has_transforms': False,\n",
      "                     'has_variables': True,\n",
      "                     'id': 'C1511847675-NSIDC_ECS',\n",
      "                     'links': [{'href': 'https://n5eil01u.ecs.nsidc.org/ATLAS/ATL06.001/',\n",
      "                                'hreflang': 'en-US',\n",
      "                                'length': '0.0KB',\n",
      "                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n",
      "                               {'href': 'http://nsidc.org/daac/subscriptions.html',\n",
      "                                'hreflang': 'en-US',\n",
      "                                'length': '0.0KB',\n",
      "                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n",
      "                               {'href': 'https://search.earthdata.nasa.gov/search/granules?p=C1511847675-NSIDC_ECS&m=-87.87967837686685!9.890967019347585!1!1!0!0%2C2&tl=1542476530!4!!&q=atl06&ok=atl06',\n",
      "                                'hreflang': 'en-US',\n",
      "                                'length': '0.0KB',\n",
      "                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n",
      "                               {'href': 'https://openaltimetry.org/',\n",
      "                                'hreflang': 'en-US',\n",
      "                                'length': '0.0KB',\n",
      "                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n",
      "                               {'href': 'https://doi.org/10.5067/ATLAS/ATL06.001',\n",
      "                                'hreflang': 'en-US',\n",
      "                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n",
      "                               {'href': 'https://doi.org/10.5067/ATLAS/ATL06.001',\n",
      "                                'hreflang': 'en-US',\n",
      "                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n",
      "                     'online_access_flag': True,\n",
      "                     'orbit_parameters': {'inclination_angle': '92.0',\n",
      "                                          'number_of_orbits': '0.071428571',\n",
      "                                          'period': '94.29',\n",
      "                                          'start_circular_latitude': '0.0',\n",
      "                                          'swath_width': '36.0'},\n",
      "                     'organizations': ['NASA NSIDC DAAC',\n",
      "                                       'NASA/GSFC/EOS/ESDIS'],\n",
      "                     'original_format': 'ISO19115',\n",
      "                     'processing_level_id': 'Level 3',\n",
      "                     'short_name': 'ATL06',\n",
      "                     'summary': 'This data set (ATL06) provides geolocated, '\n",
      "                                'land-ice surface heights (above the WGS 84 '\n",
      "                                'ellipsoid, ITRF2014 reference frame), plus '\n",
      "                                'ancillary parameters that can be used to '\n",
      "                                'interpret and assess the quality of the '\n",
      "                                'height estimates. The data were acquired by '\n",
      "                                'the Advanced Topographic Laser Altimeter '\n",
      "                                'System (ATLAS) instrument on board the Ice, '\n",
      "                                'Cloud and land Elevation Satellite-2 '\n",
      "                                '(ICESat-2) observatory.',\n",
      "                     'time_start': '2018-10-14T00:00:00.000Z',\n",
      "                     'title': 'ATLAS/ICESat-2 L3A Land Ice Height V001',\n",
      "                     'version_id': '001'}],\n",
      "          'id': 'https://cmr.earthdata.nasa.gov:443/search/collections.json?short_name=ATL06',\n",
      "          'title': 'ECHO dataset metadata',\n",
      "          'updated': '2019-06-20T22:08:01.250Z'}}\n"
     ]
    }
   ],
   "source": [
    "# Get json response from CMR collection metadata and print results. This provides high-level metadata on a data set or \"collection\", provide in json format.\n",
    "\n",
    "params = {\n",
    "    'short_name': short_name\n",
    "}\n",
    "\n",
    "cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "response = requests.get(cmr_collections_url, params=params)\n",
    "results = json.loads(response.content)\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001\n"
     ]
    }
   ],
   "source": [
    "# Find all instances of 'version_id' in metadata and print most recent version number\n",
    "\n",
    "versions = [i['version_id'] for i in results['feed']['entry']]\n",
    "latest_version = max(versions)\n",
    "print(latest_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-01T00:00:00Z,2019-05-31T23:59:59Z\n"
     ]
    }
   ],
   "source": [
    "# Input temporal range \n",
    "\n",
    "# Input start date in yyyy-MM-dd format\n",
    "start_date = '2018-06-01'\n",
    "# Input start time in HH:mm:ss format\n",
    "start_time = '00:00:00'\n",
    "# Input end date in yyyy-MM-dd format\n",
    "end_date = '2019-05-31'\n",
    "# Input end time in HH:mm:ss format\n",
    "end_time = '23:59:59'\n",
    "\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'\n",
    "print(temporal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area of Interest input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-121.83,46.8,121.71,46.92\n"
     ]
    }
   ],
   "source": [
    "# Commenting for tutorial since we will be walking through option 3 (spatial file input) together\n",
    "# Bounding Box spatial parameter in 'W,S,E,N' format\n",
    "\n",
    "# Input bounding box\n",
    "# Input lower left longitude in decimal degrees\n",
    "LL_lon = '-121.83'\n",
    "# Input lower left latitude in decimal degrees\n",
    "LL_lat =  '46.8'\n",
    "# Input upper right longitude in decimal degrees\n",
    "UR_lon = '121.71'\n",
    "# Input upper right latitude in decimal degrees\n",
    "UR_lat =  '46.92'\n",
    "\n",
    "bounding_box = LL_lon + ',' + LL_lat + ',' + UR_lon + ',' + UR_lat\n",
    "# aoi value used for CMR params below\n",
    "aoi = '1'\n",
    "print(bounding_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMR search parameters:  {'short_name': 'ATL06', 'version': '001', 'temporal': '2018-06-01T00:00:00Z,2019-05-31T23:59:59Z', 'page_size': 100, 'page_num': 1, 'bounding_box': '-121.83,46.8,121.71,46.92'}\n"
     ]
    }
   ],
   "source": [
    "#Create CMR parameters used for granule search. Modify params depending on bounding_box or polygon input.\n",
    "\n",
    "if aoi == '1':\n",
    "# bounding box input:\n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    'bounding_box': bounding_box\n",
    "    }\n",
    "else:\n",
    "    \n",
    "# If polygon input (either via coordinate pairs or shapefile/KML/KMZ):\n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    'polygon': polygon,\n",
    "    }\n",
    "\n",
    "print('CMR search parameters: ', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input the parameter dictionary to the CMR granule search to query all granules that meet the criteria based on the granule metadata. Print the number of granules returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1495"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query number of granules using our (paging over results)\n",
    "\n",
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "\n",
    "granules = []\n",
    "while True:\n",
    "    response = requests.get(granule_search_url, params=params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    params['page_num'] += 1\n",
    "\n",
    "    \n",
    "# Get number of granules over my area and time of interest\n",
    "len(granules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now query the average size of those granules: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "\n",
    "# Average size of granules in MB\n",
    "print(mean(granule_sizes))\n",
    "\n",
    "# Total volume in MB\n",
    "print(sum(granule_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the subsetting and reformatting services enabled for your data set of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query service capability URL \n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{latest_version}.xml'\n",
    "\n",
    "print(capability_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session to store cookie and pass credentials to capabilities url\n",
    "\n",
    "session = requests.session()\n",
    "s = session.get(capability_url)\n",
    "response = session.get(s.url,auth=(uid,pswd))\n",
    "\n",
    "root = ET.fromstring(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the service capability XML, we can collect lists with each service option to gather service information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect lists with each service option\n",
    "\n",
    "subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "\n",
    "# variable subsetting\n",
    "variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "\n",
    "# reformatting\n",
    "formats = [Format.attrib for Format in root.iter('Format')]\n",
    "format_vals = [formats[i]['value'] for i in range(len(formats))]\n",
    "format_vals.remove('')\n",
    "\n",
    "# reprojection only applicable on ICESat-2 L3B products, yet to be available. \n",
    "\n",
    "# reformatting options that support reprojection\n",
    "normalproj = [Projections.attrib for Projections in root.iter('Projections')]\n",
    "normalproj_vals = []\n",
    "normalproj_vals.append(normalproj[0]['normalProj'])\n",
    "format_proj = normalproj_vals[0].split(',')\n",
    "format_proj.remove('')\n",
    "format_proj.append('No reformatting')\n",
    "\n",
    "#reprojection options\n",
    "projections = [Projection.attrib for Projection in root.iter('Projection')]\n",
    "proj_vals = []\n",
    "for i in range(len(projections)):\n",
    "    if (projections[i]['value']) != 'NO_CHANGE' :\n",
    "        proj_vals.append(projections[i]['value'])\n",
    "        \n",
    "# reformatting options that do not support reprojection\n",
    "no_proj = [i for i in format_vals if i not in format_proj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's confirm that subset services exist for our data set by reviewing the `subagent` list. If the list contains service information, we know that services are available. If not, we need to set the `agent` API parameter to `NO` to indicate that our request will bypass the subsetter. This will quickly send back the data \"natively\" without any customization applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subagent)\n",
    "if len(subagent) < 1 :\n",
    "    agent = 'NO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information is contained in the subagent list, including the maximum number of granules that we can request per order depending on our configuration. We'll come back to these options below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll begin populating the subsetting and reformatting parameters used for our NSIDC API request. In addition to the CMR information we queried above, the NSIDC API accepts Key-Value-Pairs (KVPs) for subsetting and reformatting services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start with spatial subsetting. Recall that there are three options to *filter* our search results by spatial constraint: \n",
    "\n",
    "1) Bounding Box: Corresponding to the CMR `bounding_box` KVP\n",
    "\n",
    "2) Polygon coordinate pairs: Corresponding to the CMR `polygon` KVP\n",
    "\n",
    "3) Spatial file input, including Esri Shapefile or KML/KMZ: We simplified the file input to also be read by the CMR `polygon` KVP \n",
    "    \n",
    "#### We see above that `spatialSubsetting` is `true` and `spatialSubsettingShapefile` is `true`. Therefore the same *filtering* options can be applied to our *subset* constraint, with unique KVPs for the subsetting service:\n",
    "\n",
    "1) Bounding Box: `bbox` subset KVP\n",
    "\n",
    "2) Polygon coordinate pairs: `bounding_shape` subset KVP in [GeoJSON](https://geojson.org/) format. \n",
    "\n",
    "3) Spatial file input: The file can be read directly by the subsetter without simplification. This file will be posted to the API endpoint, so we don't need to specify an additional subset KVP here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because we're pursuing option 3), we don't need to provide an additional subset parameter. Below is commented code for bounding box inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bounding box subsetting (bbox) in same format as bounding_box\n",
    "\n",
    "bbox = bounding_box\n",
    "\n",
    "#Polygon coordinate pair subsetting in GeoJSON format. Or for simplicity, get polygon bounds to be used as bounding box input\n",
    "\n",
    "# # Create shapely Polygon object from x y list\n",
    "# p = Polygon(tuple(xylist))\n",
    "# # Extract the point values that define the perimeter of the polygon\n",
    "# bounds = p.bounds\n",
    "# bbox = ','.join(map(str, list(bounds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal subsetting is next, since we saw above that `temporalSubsetting` is `true`. We filtered data over 22 Feb 2019 and we can also subset the data to those dates if desired. \n",
    "\n",
    "The `time` KVP is used to subset temporally. This can be entered in the following formats:\n",
    "\n",
    "`time=yyyy-mm-dd,yyyy-mm-dd`\n",
    "\n",
    "`time=yyy-mm-ddThh:MM:ss,yyy-mm-ddThh:MM:ss` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal subsetting KVP\n",
    "\n",
    "timevar = start_date + 'T' + start_time + ',' + end_date + 'T' + end_time\n",
    "print(timevar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, let's determine if variable subsetting is available by finding the length of the `variable_vals` list we gathered from the capabilities URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(variable_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the entire list of variables if desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(variable_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can enter a list of variables to subset separated by comma using the `coverage` key. All forward slashes need to be included to indicate HDF group hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = '/ancillary_data/atlas_sdp_gps_epoch,\\\n",
    "/gt1l/land_ice_segments/atl06_quality_summary,\\\n",
    "/gt1l/land_ice_segments/delta_time,\\\n",
    "/gt1l/land_ice_segments/h_li,\\\n",
    "/gt1l/land_ice_segments/h_li_sigma,\\\n",
    "/gt1l/land_ice_segments/latitude,\\\n",
    "/gt1l/land_ice_segments/longitude,\\\n",
    "/gt1l/land_ice_segments/segment_id,\\\n",
    "/gt1l/land_ice_segments/sigma_geo_h,\\\n",
    "/gt1r/land_ice_segments/atl06_quality_summary,\\\n",
    "/gt1r/land_ice_segments/delta_time,\\\n",
    "/gt1r/land_ice_segments/h_li,\\\n",
    "/gt1r/land_ice_segments/h_li_sigma,\\\n",
    "/gt1r/land_ice_segments/latitude,\\\n",
    "/gt1r/land_ice_segments/longitude,\\\n",
    "/gt1r/land_ice_segments/segment_id,\\\n",
    "/gt1r/land_ice_segments/sigma_geo_h,\\\n",
    "/gt2l/land_ice_segments/atl06_quality_summary,\\\n",
    "/gt2l/land_ice_segments/delta_time,\\\n",
    "/gt2l/land_ice_segments/h_li,\\\n",
    "/gt2l/land_ice_segments/h_li_sigma,\\\n",
    "/gt2l/land_ice_segments/latitude,\\\n",
    "/gt2l/land_ice_segments/longitude,\\\n",
    "/gt2l/land_ice_segments/segment_id,\\\n",
    "/gt2l/land_ice_segments/sigma_geo_h,\\\n",
    "/gt2r/land_ice_segments/atl06_quality_summary,\\\n",
    "/gt2r/land_ice_segments/delta_time,\\\n",
    "/gt2r/land_ice_segments/h_li,\\\n",
    "/gt2r/land_ice_segments/h_li_sigma,\\\n",
    "/gt2r/land_ice_segments/latitude,\\\n",
    "/gt2r/land_ice_segments/longitude,\\\n",
    "/gt2r/land_ice_segments/segment_id,\\\n",
    "/gt2r/land_ice_segments/sigma_geo_h,\\\n",
    "/gt3l/land_ice_segments/atl06_quality_summary,\\\n",
    "/gt3l/land_ice_segments/delta_time,\\\n",
    "/gt3l/land_ice_segments/h_li,\\\n",
    "/gt3l/land_ice_segments/h_li_sigma,\\\n",
    "/gt3l/land_ice_segments/latitude,\\\n",
    "/gt3l/land_ice_segments/longitude,\\\n",
    "/gt3l/land_ice_segments/segment_id,\\\n",
    "/gt3l/land_ice_segments/sigma_geo_h,\\\n",
    "/gt3r/land_ice_segments/atl06_quality_summary,\\\n",
    "/gt3r/land_ice_segments/delta_time,\\\n",
    "/gt3r/land_ice_segments/h_li,\\\n",
    "/gt3r/land_ice_segments/h_li_sigma,\\\n",
    "/gt3r/land_ice_segments/latitude,\\\n",
    "/gt3r/land_ice_segments/longitude,\\\n",
    "/gt3r/land_ice_segments/segment_id,\\\n",
    "/gt3r/land_ice_segments/sigma_geo_h,\\\n",
    "/orbit_info/cycle_number,\\\n",
    "/orbit_info/rgt,\\\n",
    "/orbit_info/orbit_number' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request data from the NSIDC data access API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set NSIDC data access base URL\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of granules requested per order, which we will initially set to 10.\n",
    "page_size = 10\n",
    "\n",
    "#Determine number of pages basd on page_size and total granules. Loop requests by this value\n",
    "page_num = math.ceil(len(granules)/page_size)\n",
    "\n",
    "#Set request mode. \n",
    "request_mode = 'async'\n",
    "\n",
    "# Determine how many individual orders we will request based on the number of granules requested\n",
    "\n",
    "print(page_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After all of these KVP inputs, what does our request look like? Here's a summary of all possible KVPs that we explored, both for CMR searching and for the subsetter:\n",
    "\n",
    "#### CMR search keys:\n",
    "* `short_name=`\n",
    "* `version=`\n",
    "* `temporal=`\n",
    "* `bounding_box=`\n",
    "* `polygon=`\n",
    "\n",
    "#### Customization service keys:\n",
    "* `time=`\n",
    "* `bbox=`\n",
    "* `bounding_shape=` \n",
    "* `format=`\n",
    "* `projection=`\n",
    "* `projection_parameters=`\n",
    "* `Coverage=`\n",
    "\n",
    "#### No customization (access only):\n",
    "* `agent=`    \n",
    "* `include_meta=` \n",
    "    * `Y` by default. `N` for No metadata requested.\n",
    "\n",
    "#### Request configuration keys:\n",
    "* `request_mode=` \n",
    "* `page_size=`\n",
    "* `page_num=`\n",
    "* `token=`\n",
    "* `email=`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we were to create an API request based on our request parameters and submit into a web browser for example, here's what we end up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print API base URL + request parameters\n",
    "API_request = f'{base_url}?short_name={short_name}&version={latest_version}&temporal={temporal}&time={timevar}&polygon={polygon}&Coverage={coverage}&request_mode={request_mode}&page_size={page_size}&page_num={page_num}&token={token}&email={email}'\n",
    "print(API_request)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll also create a new dictionary of NSIDC API KVPs to be used in our subset request. Because we are looping through each page of requests, we'll add the `page_num` KVP to our dictionary within the loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_params = {\n",
    "    'short_name': short_name, \n",
    "    'version': latest_version, \n",
    "    'temporal': temporal, \n",
    "    'bbox': bbox,\n",
    "    'time': timevar,  \n",
    "    'Coverage': coverage, \n",
    "    'request_mode': request_mode, \n",
    "    'page_size': page_size,  \n",
    "    'token': token, \n",
    "    'email': email, \n",
    "    }\n",
    "print(subset_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll request the same data but without any subsetting services applied. Let's create another request parameter dictionary with the `time` and `coverage` service keys removed, and we'll add `agent=NO` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_params = {\n",
    "    'short_name': short_name, \n",
    "    'version': latest_version, \n",
    "    'temporal': temporal,\n",
    "    'bounding_box': bbox,\n",
    "    'agent' : 'NO',\n",
    "    'include_meta' : 'Y',\n",
    "    'request_mode': request_mode, \n",
    "    'page_size': page_size,  \n",
    "    'token': token, \n",
    "    'email': email, \n",
    "    }\n",
    "\n",
    "print(request_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request Data\n",
    "\n",
    "#### Finally, we'll download the data directly to this notebook directory in a new Outputs folder. The progress of each order will be reported.\n",
    "\n",
    "We'll start by creating an output folder if the folder does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(os.getcwd() + '/Outputs')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll submit our request without subsetting services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request data service for each page number, and unzip outputs\n",
    "\n",
    "for i in range(page_num):\n",
    "    page_val = i + 1\n",
    "    print('Order: ', page_val)\n",
    "    request_params.update( {'page_num': page_val} )\n",
    "    \n",
    "# For all requests other than spatial file upload, use get function\n",
    "    request = session.get(base_url, params=request_params)\n",
    "    \n",
    "    print('Request HTTP response: ', request.status_code)\n",
    "\n",
    "# Raise bad request: Loop will stop for bad response code.\n",
    "    request.raise_for_status()\n",
    "    print('Order request URL: ', request.url)\n",
    "    esir_root = ET.fromstring(request.content)\n",
    "    print('Order request response XML content: ', request.content)\n",
    "\n",
    "#Look up order ID\n",
    "    orderlist = []   \n",
    "    for order in esir_root.findall(\"./order/\"):\n",
    "        orderlist.append(order.text)\n",
    "    orderID = orderlist[0]\n",
    "    print('order ID: ', orderID)\n",
    "\n",
    "#Create status URL\n",
    "    statusURL = base_url + '/' + orderID\n",
    "    print('status URL: ', statusURL)\n",
    "\n",
    "#Find order status\n",
    "    request_response = session.get(statusURL)    \n",
    "    print('HTTP response from order response URL: ', request_response.status_code)\n",
    "    \n",
    "# Raise bad request: Loop will stop for bad response code.\n",
    "    request_response.raise_for_status()\n",
    "    request_root = ET.fromstring(request_response.content)\n",
    "    statuslist = []\n",
    "    for status in request_root.findall(\"./requestStatus/\"):\n",
    "        statuslist.append(status.text)\n",
    "    status = statuslist[0]\n",
    "    print('Data request ', page_val, ' is submitting...')\n",
    "    print('Initial request status is ', status)\n",
    "\n",
    "#Continue loop while request is still processing\n",
    "    while status == 'pending' or status == 'processing': \n",
    "        print('Status is not complete. Trying again.')\n",
    "        time.sleep(10)\n",
    "        loop_response = session.get(statusURL)\n",
    "\n",
    "# Raise bad request: Loop will stop for bad response code.\n",
    "        loop_response.raise_for_status()\n",
    "        loop_root = ET.fromstring(loop_response.content)\n",
    "\n",
    "#find status\n",
    "        statuslist = []\n",
    "        for status in loop_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Retry request status is: ', status)\n",
    "        if status == 'pending' or status == 'processing':\n",
    "            continue\n",
    "\n",
    "#Order can either complete, complete_with_errors, or fail:\n",
    "# Provide complete_with_errors error message:\n",
    "    if status == 'complete_with_errors' or status == 'failed':\n",
    "        messagelist = []\n",
    "        for message in loop_root.findall(\"./processInfo/\"):\n",
    "            messagelist.append(message.text)\n",
    "        print('error messages:')\n",
    "        pprint.pprint(messagelist)\n",
    "\n",
    "# Download zipped order if status is complete or complete_with_errors\n",
    "    if status == 'complete' or status == 'complete_with_errors':\n",
    "        downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "        print('Zip download URL: ', downloadURL)\n",
    "        print('Beginning download of zipped output...')\n",
    "        zip_response = session.get(downloadURL)\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        zip_response.raise_for_status()\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "            z.extractall(path)\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "    else: print('Request failed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our request loop again, this time with subsetting services applied. We will post the KML file directly to the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request data service for each page number, and unzip outputs\n",
    "\n",
    "for i in range(page_num):\n",
    "    page_val = i + 1\n",
    "    print('Order: ', page_val)\n",
    "    subset_params.update( {'page_num': page_val} )\n",
    "    \n",
    "# Post polygon to API endpoint for polygon subsetting to subset based on original, non-simplified KML file\n",
    "\n",
    "#    shape_post = {'shapefile': open(kml_filepath, 'rb')}\n",
    "#    request = session.post(base_url, params=subset_params, files=shape_post) \n",
    "    \n",
    "# FOR ALL OTHER REQUESTS THAT DO NOT UTILIZED AN UPLOADED POLYGON FILE, USE A GET REQUEST INSTEAD OF POST:\n",
    "    request = session.get(base_url, params=request_params)\n",
    "    \n",
    "    print('Request HTTP response: ', request.status_code)\n",
    "\n",
    "# Raise bad request: Loop will stop for bad response code.\n",
    "    request.raise_for_status()\n",
    "    print('Order request URL: ', request.url)\n",
    "    esir_root = ET.fromstring(request.content)\n",
    "    print('Order request response XML content: ', request.content)\n",
    "\n",
    "# Look up order ID\n",
    "    orderlist = []   \n",
    "    for order in esir_root.findall(\"./order/\"):\n",
    "        orderlist.append(order.text)\n",
    "    orderID = orderlist[0]\n",
    "    print('order ID: ', orderID)\n",
    "\n",
    "# Create status URL\n",
    "    statusURL = base_url + '/' + orderID\n",
    "    print('status URL: ', statusURL)\n",
    "\n",
    "# Find order status\n",
    "    request_response = session.get(statusURL)    \n",
    "    print('HTTP response from order response URL: ', request_response.status_code)\n",
    "    \n",
    "# Raise bad request: Loop will stop for bad response code.\n",
    "    request_response.raise_for_status()\n",
    "    request_root = ET.fromstring(request_response.content)\n",
    "    statuslist = []\n",
    "    for status in request_root.findall(\"./requestStatus/\"):\n",
    "        statuslist.append(status.text)\n",
    "    status = statuslist[0]\n",
    "    print('Data request ', page_val, ' is submitting...')\n",
    "    print('Initial request status is ', status)\n",
    "\n",
    "# Continue to loop while request is still processing\n",
    "    while status == 'pending' or status == 'processing': \n",
    "        print('Status is not complete. Trying again.')\n",
    "        time.sleep(10)\n",
    "        loop_response = session.get(statusURL)\n",
    "\n",
    "# Raise bad request: Loop will stop for bad response code.\n",
    "        loop_response.raise_for_status()\n",
    "        loop_root = ET.fromstring(loop_response.content)\n",
    "\n",
    "# Find status\n",
    "        statuslist = []\n",
    "        for status in loop_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Retry request status is: ', status)\n",
    "        if status == 'pending' or status == 'processing':\n",
    "            continue\n",
    "\n",
    "# Order can either complete, complete_with_errors, or fail:\n",
    "# Provide complete_with_errors error message:\n",
    "    if status == 'complete_with_errors' or status == 'failed':\n",
    "        messagelist = []\n",
    "        for message in loop_root.findall(\"./processInfo/\"):\n",
    "            messagelist.append(message.text)\n",
    "        print('error messages:')\n",
    "        pprint.pprint(messagelist)\n",
    "\n",
    "# Download zipped order if status is complete or complete_with_errors\n",
    "    if status == 'complete' or status == 'complete_with_errors':\n",
    "        downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "        print('Zip download URL: ', downloadURL)\n",
    "        print('Beginning download of zipped output...')\n",
    "        zip_response = session.get(downloadURL)\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        zip_response.raise_for_status()\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "            z.extractall(path)\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "    else: print('Request failed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up the Output folder by removing individual order folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up Outputs folder by removing individual granule folders \n",
    "\n",
    "for root, dirs, files in os.walk(path, topdown=False):\n",
    "    for file in files:\n",
    "        try:\n",
    "            shutil.move(os.path.join(root, file), path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        \n",
    "for root, dirs, files in os.walk(path):\n",
    "    for name in dirs:\n",
    "        os.rmdir(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List files\n",
    "sorted(os.listdir(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in the streaming request method, an example loop is below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set page size to 1 to improve performance\n",
    "page_size = 1\n",
    "request_params.update( {'page_size': page_size})\n",
    "\n",
    "# No metadata to only return a single output\n",
    "request_params.update( {'include_meta': 'N'})\n",
    "\n",
    "#Determine number of pages basd on page_size and total granules. Loop requests by this value\n",
    "page_num = math.ceil(len(granules)/page_size)\n",
    "print(page_num)\n",
    "\n",
    "#Set request mode. \n",
    "request_params.update( {'request_mode': 'stream'})\n",
    "\n",
    "print(request_params)\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "for i in range(page_num):\n",
    "    page_val = i + 1\n",
    "    print('Order: ', page_val)\n",
    "    request_params.update( {'page_num': page_val})\n",
    "    request = session.get(base_url, params=request_params)\n",
    "    print('HTTP response from order response URL: ', request.status_code)\n",
    "    request.raise_for_status()\n",
    "    d = request.headers['content-disposition']\n",
    "    fname = re.findall('filename=(.+)', d)\n",
    "    open(eval(fname[0]), 'wb').write(request.content)\n",
    "    print('Data request', page_val, 'is complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To review, we have explored data availability and volume over a region and time of interest, discovered and selected data customization options, and downloaded data directly to our Pangeo environment. You are welcome to modify the search and service parameters to submit more requests to NSIDC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
